# Fine-Tuning-LLMs

This repository contains Jupyter notebooks for supervised fine-tuning of various state-of-the-art language models: LLaMA2, LLaMA3, Gemma2, and Qwen2.5. Each notebook demonstrates the steps for loading a model, preparing the dataset, fine-tuning, and evaluating the results.

## Models Fine-Tuned
1. **LLaMA2**
2. **LLaMA3**
3. **Gemma2**
4. **Qwen2.5**

### Overview

Fine-tuning large language models (LLMs) on custom datasets allows us to enhance the model's performance for specific tasks. The notebooks in this repository walk through the supervised fine-tuning process, including:

- **Dataset preparation**: Tokenizing and formatting data for supervised training.
- **Training**: Using standard fine-tuning techniques on pre-trained models.
- **Evaluation**: Generating metrics to compare model performance.
